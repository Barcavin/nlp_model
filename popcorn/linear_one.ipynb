{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                      ] 0.12%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/kevindong1994/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ =============        ] 65.92%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ===================  ] 100.00%"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "train = pd.read_csv(\"./training_data.csv\", header=0, delimiter=\"\\t|\\n\")\n",
    "data = train[\"comment\"]\n",
    "### Text Preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "keep_stopwords = stopwords.words(\"english\")[-36:]+stopwords.words(\"english\")[131:133] + stopwords.words(\"english\")[116:120]\n",
    "new_stopwords = set(stopwords.words(\"english\")).difference(set(keep_stopwords))\n",
    "replace_num = \"[-+]?[0-9]*\\.?[0-9]+\"\n",
    "replace_url = \"http(s?)://[^\\s]+\"\n",
    "def review_to_words(raw_review):\n",
    "    \n",
    "    review_text = BeautifulSoup(raw_review).get_text()\n",
    "    review_text = re.sub(r'[\\t\\r\\n]',\" \",review_text)\n",
    "    url = re.sub(replace_url,\"URL\",raw_review)\n",
    "    num = re.sub(replace_num,\"NUM\",url)\n",
    "    letters_only = re.sub(\"[^a-zA-Z']\", \" \", num) #Also keep single quote\n",
    "    \n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    meaningful_words = [w for w in words if not w in new_stopwords]\n",
    "    \n",
    "    return(\" \".join(meaningful_words))\n",
    "\n",
    "clean_data = dict()    \n",
    "clean_data[\"x\"] = []\n",
    "clean_data[\"y\"] = []\n",
    "import bar\n",
    "step=0\n",
    "total = len(data)\n",
    "for each in data:\n",
    "    clean_data[\"x\"].append(review_to_words(each))\n",
    "    bar.drawProgressBar(step/total)\n",
    "    step +=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data[\"y\"] = train[\"score\"]\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_data[\"x\"], clean_data[\"y\"], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1          7\n",
       "2         10\n",
       "3          7\n",
       "4          6\n",
       "5          3\n",
       "6          7\n",
       "7          7\n",
       "8          7\n",
       "9          6\n",
       "10         5\n",
       "11         1\n",
       "12         8\n",
       "13         7\n",
       "14         5\n",
       "15         8\n",
       "16        10\n",
       "17         7\n",
       "18        10\n",
       "19         8\n",
       "20        10\n",
       "21        10\n",
       "22        10\n",
       "23         8\n",
       "24         8\n",
       "25         8\n",
       "26         5\n",
       "27        10\n",
       "28         9\n",
       "29         0\n",
       "          ..\n",
       "151794     3\n",
       "151795     7\n",
       "151796     3\n",
       "151797     3\n",
       "151798     5\n",
       "151799     1\n",
       "151800     3\n",
       "151801     1\n",
       "151802    10\n",
       "151803     8\n",
       "151804    10\n",
       "151805     9\n",
       "151806     9\n",
       "151807    10\n",
       "151808     6\n",
       "151809     4\n",
       "151810    10\n",
       "151811    10\n",
       "151812     7\n",
       "151813    10\n",
       "151814    10\n",
       "151815    10\n",
       "151816    10\n",
       "151817    10\n",
       "151818    10\n",
       "151819    10\n",
       "151820    10\n",
       "151821    10\n",
       "151822     9\n",
       "151823    10\n",
       "Name: score, Length: 151824, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "num_features = 500\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = num_features) \n",
    "train_data_features = vectorizer.fit_transform(X_train)\n",
    "train_data_features = train_data_features.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "epochs = 50\n",
    "epoch = 0\n",
    "ids = list(range(0,train_data_features.shape[0],batch_size))\n",
    "i=0\n",
    "def generate_batch(features,labels):\n",
    "    global i\n",
    "    global ids\n",
    "    global epoch\n",
    "    if i==len(ids)-1:\n",
    "        inner_i = i\n",
    "        i = 0\n",
    "        epoch += 1\n",
    "        return features[ids[inner_i]:],labels[ids[inner_i]:]\n",
    "    else:\n",
    "        inner_i = i\n",
    "        i +=1\n",
    "        return features[ids[inner_i]:ids[i]],labels[ids[inner_i]:ids[i]]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "H = 300\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32,shape=[None,num_features])\n",
    "    y_ = tf.placeholder(tf.float32,shape=[None,1])\n",
    "    fc_l1 = tf.layers.dense(x,units=H,activation=tf.nn.relu, kernel_initializer=tf.random_normal_initializer(),\\\n",
    "                     kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0))\n",
    "    output = tf.layers.dense(fc_l1,units=1,kernel_initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(output,new_y)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    opt_op = opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session(graph = graph) as session:\n",
    "\n",
    "session = tf.Session(graph = graph)\n",
    "session.run(init)\n",
    "step = 0 \n",
    "print(\"Initialized\")\n",
    "while epoch<epochs:\n",
    "    step+=1\n",
    "    batch_x,batch_y = generate_batch(train_data,y_train)\n",
    "    feed_dict={x:batch_x,y_:batch_y}\n",
    "    _,loss_val = session.run([opt_op,loss],feed_dict=feed_dict)\n",
    "    if step % 2000 == 0:\n",
    "        print(\"Step: %i, Epoch: %i, Loss:%f\"%(step,epoch,loss_val))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

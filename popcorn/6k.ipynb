{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import tensorflow\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter\n",
    "from pipe import transform_text_func,FeatureExtractor, ImputeNA, CategoricalEncoding,text\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, make_scorer,mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import csr_matrix\n",
    "import logging\n",
    "from sklearn.pipeline import make_pipeline, make_union \n",
    "#from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation, BatchNormalization, PReLU\n",
    "#from keras.initializers import he_uniform\n",
    "#from keras.layers import Conv1D\n",
    "#from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "#from keras.optimizers import Adam, SGD\n",
    "#from keras.models import Model\n",
    "#import gc\n",
    "# from gensim import corpora,models,similarities\n",
    "# import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tuning.py',\n",
       " '.ipynb_checkpoints',\n",
       " 'training_data.csv',\n",
       " 'tfidf.ipynb',\n",
       " 'bow_randomforest.ipynb',\n",
       " 'pipe.py',\n",
       " 'Maybe Success!!!!.ipynb',\n",
       " 'lda.py',\n",
       " 'linear_one.ipynb',\n",
       " '6k.ipynb',\n",
       " 'word2vec.py',\n",
       " 'bar.py',\n",
       " 'failure.ipynb',\n",
       " 'ridge_test_res.pkl',\n",
       " '__pycache__',\n",
       " 'bow.py',\n",
       " 'yelp.py',\n",
       " 'fastText.ipynb',\n",
       " 'Untitled1.ipynb',\n",
       " 'dataset',\n",
       " 'tuning.ipynb',\n",
       " 'README.md']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('training_data.csv',header= 0 ,delimiter='\\t|\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------generate interaction feature between cate-------\n",
      "time elapsed:  39.21766400337219\n"
     ]
    }
   ],
   "source": [
    "print('-------generate interaction feature between cate-------')\n",
    "s = time.time()\n",
    "a = ['age_cat','sex','stay_cat','lang','er','category']\n",
    "\n",
    "for i,ai in enumerate(a):\n",
    "    for j,bj in enumerate(a):\n",
    "        if i<j:\n",
    "            x = train[ai]\n",
    "            y = train[bj]\n",
    "            t = []\n",
    "            for l in range(train.shape[0]):\n",
    "                t.append(str(x[l])+' '+ str(y[l]))\n",
    "            train[ai+'_'+bj] = t\n",
    "print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total = list(train.comment)\n",
    "y_total = list(train.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------calculate uppercase prob to features-------\n",
      "time elapsed:  1.3343238830566406\n",
      "-------choose specific stop words-------\n",
      "total vocab:  44702\n",
      "vocab size frequency > 1:  20256\n",
      "time elapsed:  1.6845476627349854\n"
     ]
    }
   ],
   "source": [
    "print('-------calculate uppercase prob to features-------')\n",
    "s = time.time()\n",
    "uppercase = []\n",
    "for i in x_total:\n",
    "    length = len(i.split())\n",
    "    tmp = []\n",
    "    for j in i:\n",
    "        if j.isupper():\n",
    "            tmp.append(j)\n",
    "    uppercase.append(len(tmp)/length)\n",
    "uppercase = np.array(uppercase).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_up = scaler.fit_transform(uppercase)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------choose specific stop words-------')\n",
    "s=time.time()\n",
    "x_lower = [sublist.lower() for sublist in x_total]\n",
    "# punctuation = string.punctuation.replace('!','').replace('?','').replace(\"'\",'').replace('~','')\n",
    "# regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "def tokenize(text):\n",
    "#     text = regex.sub(\" \", text) # remove punctuation\n",
    "    text = text.replace('w/','with ')\n",
    "#     text = text.replace('&',' and ')\n",
    "#     text = text.replace('/',' or ')\n",
    "    text = text.replace('.',' ')\n",
    "    text = text.split()\n",
    "    return(text)\n",
    "x_lower = [tokenize(i) for i in x_lower]\n",
    "x_unlist = []\n",
    "for i in x_lower:\n",
    "    x_unlist += i\n",
    "vocab_dic = Counter(x_unlist)\n",
    "stopwords_num = 250\n",
    "# print([i[0] for i in vocab_dic.most_common(stopwords_num)],'\\n\\n')\n",
    "print('total vocab: ',len(vocab_dic.most_common()))\n",
    "maxfeature = len([i[0] for i in vocab_dic.most_common() if i[1]>1])\n",
    "print('vocab size frequency > 1: ', maxfeature)\n",
    "\n",
    "stop = [i[0] for i in vocab_dic.most_common(stopwords_num)]\n",
    "from itertools import compress\n",
    "x_5level = [list(compress(x_lower, list(np.array(y_total)==i))) for i in np.unique(y_total)]\n",
    "\n",
    "x_5level_unlist = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "for i in range(len(x_5level)):\n",
    "    for j in x_5level[i]:\n",
    "        x_5level_unlist[i] += j\n",
    "multilevel_vocab = []\n",
    "for i in range(11):\n",
    "    multilevel_vocab.append(Counter(x_5level_unlist[i]))\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "multilevel_stop = defaultdict(list)\n",
    "for i in range(11):\n",
    "    tt = len(x_5level_unlist[i])\n",
    "    for j in stop:\n",
    "        multilevel_stop[j].append(multilevel_vocab[i][j]/tt)\n",
    "\n",
    "stop_var = [(key,np.std(value)*1000) for key,value in multilevel_stop.items() ]\n",
    "stop = [i[0] for i in stop_var if i[1]<0.5]\n",
    "\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        punctuation = string.punctuation.replace('!','').replace('?','').replace(\"'\",'').replace(\"/\",'').replace(\"@\",'').replace('\"','')\n",
    "        regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        text = text.replace('w/',' with ')\n",
    "        text = text.replace('&',' and ')\n",
    "        text = text.replace(\"/\",' or ')\n",
    "        text = text.replace('\"\"',' sarcasm ')\n",
    "        text = text.replace(\"'d\",' ')\n",
    "        text = text.replace(\"'s\",' ')\n",
    "        text = text.replace(\"'re\",' ')\n",
    "        text = text.replace(\"'ll\",' ')\n",
    "        text = text.replace(\"'ve\",' ')    \n",
    "#         text = text.replace('.0','')\n",
    "        text = text.replace('.',' ')\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        tokens = []\n",
    "        tokens_ = [s.split() for s in sent_tokenize(text)]\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent   \n",
    "        filtered_tokens = [ps.stem(w.lower()) for w in tokens]\n",
    "        return filtered_tokens\n",
    "    except TypeError as e: print(text,e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------transform to features-------\n",
      "time elapsed:  61.7270393371582\n",
      "-------add length and upper prob to features-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (151824, 20702)\n",
      "time elapsed:  30.358232736587524\n"
     ]
    }
   ],
   "source": [
    "print('-------transform to features-------')\n",
    "s=time.time()\n",
    "onehot_list = ['age_cat', 'sex', 'stay_cat', 'lang', 'er','age_cat_sex', 'age_cat_stay_cat',\n",
    "       'age_cat_lang', 'age_cat_er', 'age_cat_category', 'sex_stay_cat',\n",
    "       'sex_lang', 'sex_er', 'sex_category', 'stay_cat_lang', 'stay_cat_er',\n",
    "       'stay_cat_category', 'lang_er', 'lang_category', 'er_category']\n",
    "onehot_pipeline = make_pipeline(FeatureExtractor(onehot_list),\n",
    "                                CategoricalEncoding('OneHot'),\n",
    "                                )\n",
    "descrip_pipeline = make_pipeline(FeatureExtractor('comment'),\n",
    "                                text(method='tfidf', ngram = 3, max_f = maxfeature, \n",
    "                                     binary = True, stopwords=stop,tokenizer=tokenize,analyzer ='word'))\n",
    "\n",
    "feature_union = make_union(\n",
    "    onehot_pipeline,\n",
    "    descrip_pipeline\n",
    ")\n",
    "X = feature_union.fit_transform(train)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------add length and upper prob to features-------')\n",
    "s = time.time()\n",
    "length = np.array([(X[i,]!=0).sum() for i in range(X.shape[0])]).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_l = scaler.fit_transform(length)\n",
    "X = hstack([X,new_l],format='csr')\n",
    "X = hstack([X,new_up],format='csr')\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((151824, 20702), 151824)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,len(y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def int2vector(i):\n",
    "    return np.array([1]*(i+1)+[0]*(10-i)).reshape(11,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_total, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train = [int2vector(i) for i in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature:20702,H:5,batch:50\n",
      "Initialized\n",
      "Step: 2000, Epoch: 0, Loss:0.087136\n",
      "Step: 4000, Epoch: 1, Loss:0.079032\n",
      "Step: 6000, Epoch: 2, Loss:0.093925\n",
      "Step: 8000, Epoch: 3, Loss:0.081407\n",
      "Step: 10000, Epoch: 4, Loss:0.058652\n",
      "Step: 12000, Epoch: 5, Loss:0.060201\n",
      "Step: 14000, Epoch: 6, Loss:0.055470\n",
      "Step: 16000, Epoch: 7, Loss:0.047501\n",
      "Step: 18000, Epoch: 8, Loss:0.060264\n",
      "Step: 20000, Epoch: 9, Loss:0.054659\n",
      "Step: 22000, Epoch: 10, Loss:0.046703\n",
      "Step: 24000, Epoch: 11, Loss:0.047577\n",
      "Step: 26000, Epoch: 12, Loss:0.049691\n",
      "Step: 28000, Epoch: 13, Loss:0.044234\n",
      "Step: 30000, Epoch: 14, Loss:0.039882\n",
      "Step: 32000, Epoch: 15, Loss:0.040288\n",
      "Step: 34000, Epoch: 16, Loss:0.039945\n",
      "Step: 36000, Epoch: 17, Loss:0.046995\n",
      "Step: 38000, Epoch: 18, Loss:0.036522\n",
      "Step: 40000, Epoch: 19, Loss:0.040401\n",
      "Step: 42000, Epoch: 20, Loss:0.039139\n",
      "Step: 44000, Epoch: 21, Loss:0.039325\n",
      "Step: 46000, Epoch: 22, Loss:0.053046\n",
      "Step: 48000, Epoch: 23, Loss:0.038395\n",
      "Step: 50000, Epoch: 24, Loss:0.039745\n",
      "Step: 52000, Epoch: 25, Loss:0.044316\n",
      "Step: 54000, Epoch: 26, Loss:0.034419\n",
      "Step: 56000, Epoch: 27, Loss:0.033422\n",
      "Step: 58000, Epoch: 28, Loss:0.032709\n",
      "Step: 60000, Epoch: 29, Loss:0.032334\n",
      "Step: 62000, Epoch: 30, Loss:0.028885\n",
      "Step: 64000, Epoch: 31, Loss:0.045065\n",
      "Step: 66000, Epoch: 32, Loss:0.053532\n",
      "Step: 68000, Epoch: 33, Loss:0.036006\n",
      "Step: 70000, Epoch: 34, Loss:0.036068\n",
      "Step: 72000, Epoch: 35, Loss:0.034602\n",
      "Step: 74000, Epoch: 36, Loss:0.039798\n",
      "Step: 76000, Epoch: 37, Loss:0.039473\n",
      "Step: 78000, Epoch: 38, Loss:0.046011\n",
      "Step: 80000, Epoch: 39, Loss:0.053040\n",
      "Step: 82000, Epoch: 40, Loss:0.066186\n",
      "Step: 84000, Epoch: 41, Loss:0.039451\n",
      "Step: 86000, Epoch: 42, Loss:0.028796\n",
      "Step: 88000, Epoch: 43, Loss:0.048554\n",
      "Step: 90000, Epoch: 44, Loss:0.032415\n",
      "Step: 92000, Epoch: 45, Loss:0.035912\n",
      "Step: 94000, Epoch: 46, Loss:0.046513\n",
      "Step: 96000, Epoch: 47, Loss:0.034838\n",
      "Step: 98000, Epoch: 48, Loss:0.037026\n",
      "Step: 100000, Epoch: 49, Loss:0.032441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "def to_predict(row):\n",
    "        for i in range(11):\n",
    "            if row[-1]>0.5:\n",
    "                return 10\n",
    "            elif row[i]<0.5:\n",
    "                return i-1\n",
    "\n",
    "logging = {\"i\":0,\"epoch\":0,\"epochs\":50}            \n",
    "            \n",
    "def generate_batch(features,labels,ids):\n",
    "    global logging\n",
    "    if logging[\"i\"]==len(ids)-1:\n",
    "        inner_i = logging[\"i\"]\n",
    "        logging[\"i\"] = 0\n",
    "        logging[\"epoch\"] += 1\n",
    "        return features[ids[inner_i]:],labels[ids[inner_i]:]\n",
    "    else:\n",
    "        inner_i = logging[\"i\"]\n",
    "        logging[\"i\"] += 1\n",
    "        return features[ids[inner_i]:ids[logging[\"i\"]]],labels[ids[inner_i]:ids[logging[\"i\"]]]\n",
    "\n",
    "\n",
    "\n",
    "def sum_to_pred(row):\n",
    "    r = np.rint(np.sum(row))\n",
    "    if r>10:\n",
    "        return 10\n",
    "    elif r<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "train_data = [X_train[i,] for i in range(X_train.shape[0])]\n",
    "#train_data = [np.array(each.todense())[0] for each in train_data]\n",
    "test_data = X_test\n",
    "test_data_features = [X_test[i,] for i in range(X_test.shape[0])]\n",
    "#test_data_features = [np.array(each.todense())[0] for each in test_data_features]\n",
    "\n",
    "batch_size = 50\n",
    "num_features=X_train.shape[1]\n",
    "H = 5\n",
    "\n",
    "logging[\"i\"] = 0\n",
    "logging[\"epoch\"] = 0\n",
    "ids = list(range(0,X_train.shape[0],batch_size))\n",
    "print(\"feature:%i,H:%i,batch:%i\"%(num_features,H,batch_size))\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32,shape=[None,num_features])\n",
    "    y_ = tf.placeholder(tf.float32,shape=[None,11,1])\n",
    "    new_y = tf.squeeze(y_)\n",
    "    W_1 = tf.Variable(tf.random_uniform([num_features,H]))\n",
    "    b_1 = tf.Variable(tf.zeros([H]))\n",
    "\n",
    "    W_2 = tf.Variable(tf.random_uniform([H,11]))\n",
    "    b_2 = tf.Variable(tf.zeros([11]))\n",
    "\n",
    "    temp_1 = tf.nn.relu(tf.matmul(x,W_1,a_is_sparse=True) + b_1)\n",
    "    temp_2 = tf.matmul(temp_1,W_2) + b_2\n",
    "    output = tf.nn.sigmoid(temp_2)\n",
    "    #loss = tf.reduce_mean(tf.square(tf.subtract(output,new_y)))\n",
    "    loss = tf.losses.mean_squared_error(output,new_y)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    opt_op = opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "# with tf.Session(graph = graph) as session:\n",
    "\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "sess.run(init)\n",
    "print(\"Initialized\")\n",
    "step = 0 \n",
    "while logging[\"epoch\"]<logging[\"epochs\"]:\n",
    "    step+=1\n",
    "    batch_x,batch_y = generate_batch(train_data,y_train,ids)\n",
    "    batch_x = [np.array(each.todense())[0] for each in batch_x]\n",
    "    feed_dict={x:batch_x,y_:batch_y}\n",
    "    _,loss_val = sess.run([opt_op,loss],feed_dict=feed_dict)\n",
    "    if step % 2000 == 0:\n",
    "        print(\"Step: %i, Epoch: %i, Loss:%f\"%(step,logging[\"epoch\"],loss_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Finished\n",
      "5.266436469602012\n"
     ]
    }
   ],
   "source": [
    "test_data_features = [np.array(each.todense())[0] for each in test_data_features]\n",
    "result = sess.run(output,feed_dict={x:test_data_features})\n",
    "print(\"Calculated Finished\")\n",
    "#result = sess.run(output,feed_dict={x:test_data_features})\n",
    "result_label = [to_predict(each) for each in result]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sd = mean_squared_error(result_label, y_test)\n",
    "print(sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 23 stored elements in Compressed Sparse Row format>,\n",
       " <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 48 stored elements in Compressed Sparse Row format>,\n",
       " <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 25 stored elements in Compressed Sparse Row format>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = mord.OrdinalRidge(max_iter=100000,solver=\"sag\",tol=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_total, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrdinalRidge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=100000,\n",
       "       normalize=False, random_state=None, solver='sag', tol=0.0001)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4155123547962156"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e6f5fe8335d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopt_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step: %i, Epoch: %i, Loss:%f\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.sparse_placeholder(tf.float32,shape=[None,1,num_features])\n",
    "    y_ = tf.placeholder(tf.float32,shape=[None,11,1])\n",
    "    new_y = tf.squeeze(y_)\n",
    "    W_1 = tf.Variable(tf.random_uniform([num_features,H]))\n",
    "    b_1 = tf.Variable(tf.zeros([H]))\n",
    "\n",
    "    W_2 = tf.Variable(tf.random_uniform([H,11]))\n",
    "    b_2 = tf.Variable(tf.zeros([11]))\n",
    "\n",
    "    temp_1 = tf.nn.relu(tf.matmul(tf.squeeze(tf.sparse_tensor_to_dense(x)),W_1,a_is_sparse=True) + b_1)\n",
    "    temp_2 = tf.matmul(temp_1,W_2) + b_2\n",
    "    output = tf.nn.sigmoid(temp_2)\n",
    "    #loss = tf.reduce_mean(tf.square(tf.subtract(output,new_y)))\n",
    "    loss = tf.losses.mean_squared_error(output,new_y)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    opt_op = opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "# with tf.Session(graph = graph) as session:\n",
    "\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "sess.run(init)\n",
    "print(\"Initialized\")\n",
    "step = 0 \n",
    "while logging[\"epoch\"]<logging[\"epochs\"]:\n",
    "    step+=1\n",
    "    batch_x,batch_y = generate_batch(train_data,y_train,ids)\n",
    "    feed_dict={x:batch_x,y_:batch_y}\n",
    "    _,loss_val = sess.run([opt_op,loss],feed_dict=feed_dict)\n",
    "    if step % 2000 == 0:\n",
    "        print(\"Step: %i, Epoch: %i, Loss:%f\"%(step,logging[\"epoch\"],loss_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        -0.8488746 ,  0.5922257 ],\n",
       "       [ 0.        ,  0.        ,  1.        , ...,  0.        ,\n",
       "        -0.37600356,  2.0009363 ],\n",
       "       [ 0.        ,  1.        ,  0.        , ...,  0.        ,\n",
       "         0.99006844,  0.07322705],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        -0.48108602,  0.24004802],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "        -0.06075616, -0.34691468],\n",
       "       [ 0.        ,  0.        ,  1.        , ...,  0.        ,\n",
       "         0.3595737 , -0.46430722]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32,shape=[None,num_features])\n",
    "    y_ = tf.placeholder(tf.float32,shape=[None,11,1])\n",
    "    new_y = tf.squeeze(y_)\n",
    "    W_1 = tf.Variable(tf.random_uniform([num_features,H]))\n",
    "    b_1 = tf.Variable(tf.zeros([H]))\n",
    "\n",
    "    W_2 = tf.Variable(tf.random_uniform([H,11]))\n",
    "    b_2 = tf.Variable(tf.zeros([11]))\n",
    "\n",
    "    temp_1 = tf.nn.relu(tf.matmul(x,W_1) + b_1)\n",
    "    temp_2 = tf.matmul(temp_1,W_2) + b_2\n",
    "    output = tf.nn.sigmoid(temp_2)\n",
    "    #loss = tf.reduce_mean(tf.square(tf.subtract(output,new_y)))\n",
    "    loss = tf.losses.mean_squared_error(output,new_y)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    opt_op = opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "# with tf.Session(graph = graph) as session:\n",
    "\n",
    "batch_x,batch_y = generate_batch(train_data,y_train,ids)\n",
    "batch_x = [np.array(each.todense())[0] for each in batch_x]\n",
    "feed_={x:batch_x,y_:batch_y}\n",
    "\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "sess.run(init)\n",
    "sess.run(x,feed_dict=feed_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f128ead45f8>: [<1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 36 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 37 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 25 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 52 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 35 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 40 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 24 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 67 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 25 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 31 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 43 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 23 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 26 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 45 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 78 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 28 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 24 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 56 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 67 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 23 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 34 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 23 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 59 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 25 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 35 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 46 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 32 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 34 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 30 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 37 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 42 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 85 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 31 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 40 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 154 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 23 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 38 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 39 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 58 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 33 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 30 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 41 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 40 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 32 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 23 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 44 stored elements in Compressed Sparse Row format>,\n",
       "  <1x20702 sparse matrix of type '<class 'numpy.float64'>'\n",
       "  \twith 46 stored elements in Compressed Sparse Row format>],\n",
       " <tf.Tensor 'Placeholder_3:0' shape=(?, 11, 1) dtype=float32>: [array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1]]), array([[1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]])]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features = [np.array(each.todense())[0] for each in test_data_features]\n",
    "result = sess.run(output,feed_dict={x:test_data_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.90679014809788\n"
     ]
    }
   ],
   "source": [
    "#result = sess.run(output,feed_dict={x:test_data_features})\n",
    "result_label = [to_predict(each) for each in result]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sd = mean_squared_error(result_label, y_test)\n",
    "print(sd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = sess.run(output,feed_dict={x:test_data_features})\n",
    "result_label = [sum_to_pred(each) for each in result]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "custom = mean_squared_error(result_label, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "def to_predict(row):\n",
    "        for i in range(11):\n",
    "            if row[-1]>0.5:\n",
    "                return 10\n",
    "            elif row[i]<0.5:\n",
    "                return i-1\n",
    "\n",
    "logging = {\"i\":0,\"epoch\":0,\"epochs\":20}            \n",
    "            \n",
    "def generate_batch(features,labels,ids):\n",
    "    global logging\n",
    "    if logging[\"i\"]==len(ids)-1:\n",
    "        inner_i = logging[\"i\"]\n",
    "        logging[\"i\"] = 0\n",
    "        logging[\"epoch\"] += 1\n",
    "        return features[ids[inner_i]:],labels[ids[inner_i]:]\n",
    "    else:\n",
    "        inner_i = logging[\"i\"]\n",
    "        logging[\"i\"] += 1\n",
    "        return features[ids[inner_i]:ids[logging[\"i\"]]],labels[ids[inner_i]:ids[logging[\"i\"]]]\n",
    "\n",
    "\n",
    "\n",
    "def sum_to_pred(row):\n",
    "    r = np.rint(np.sum(row))\n",
    "    if r>10:\n",
    "        return 10\n",
    "    elif r<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "train_data = [X_train[i,] for i in range(X_train.shape[0])]\n",
    "train_data = [np.array(each.todense())[0] for each in train_data]\n",
    "test_data = X_test\n",
    "test_data_features = [X_test[i,] for i in range(X_test.shape[0])]\n",
    "test_data_features = [np.array(each.todense())[0] for each in test_data_features]\n",
    "    \n",
    "def tuning(H,batch_size,num_features=X_train.shape[1]):   \n",
    "    global logging\n",
    "    logging[\"i\"] = 0\n",
    "    logging[\"epoch\"] = 0\n",
    "    ids = list(range(0,X_train.shape[0],batch_size))\n",
    "    print(\"feature:%i,H:%i,batch:%i\"%(num_features,H,batch_size))\n",
    "    #num_features = 5000\n",
    "\n",
    "    #batch_size = 10\n",
    "\n",
    "    #H = 5\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        x = tf.placeholder(tf.float32,shape=[None,num_features])\n",
    "        y_ = tf.placeholder(tf.float32,shape=[None,11,1])\n",
    "        new_y = tf.squeeze(y_)\n",
    "        W_1 = tf.Variable(tf.random_uniform([num_features,H]))\n",
    "        b_1 = tf.Variable(tf.zeros([H]))\n",
    "\n",
    "        W_2 = tf.Variable(tf.random_uniform([H,11]))\n",
    "        b_2 = tf.Variable(tf.zeros([11]))\n",
    "\n",
    "        temp_1 = tf.nn.relu(tf.matmul(x,W_1) + b_1)\n",
    "        temp_2 = tf.matmul(temp_1,W_2) + b_2\n",
    "        output = tf.nn.sigmoid(temp_2)\n",
    "        #loss = tf.reduce_mean(tf.square(tf.subtract(output,new_y)))\n",
    "        loss = tf.losses.mean_squared_error(output,new_y)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        opt_op = opt.minimize(loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    # with tf.Session(graph = graph) as session:\n",
    "\n",
    "    sess = tf.InteractiveSession(graph=graph)\n",
    "    sess.run(init)\n",
    "    print(\"Initialized\")\n",
    "    step = 0 \n",
    "    while logging[\"epoch\"]<logging[\"epochs\"]:\n",
    "        step+=1\n",
    "        batch_x,batch_y = generate_batch(train_data,y_train,ids)\n",
    "        feed_dict={x:batch_x,y_:batch_y}\n",
    "        _,loss_val = sess.run([opt_op,loss],feed_dict=feed_dict)\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: %i, Epoch: %i, Loss:%f\"%(step,logging[\"epoch\"],loss_val))\n",
    "\n",
    "    result = sess.run(output,feed_dict={x:test_data_features})\n",
    "    result_label = [to_predict(each) for each in result]\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    sd = mean_squared_error(result_label, y_test)\n",
    "\n",
    "\n",
    "    result = sess.run(output,feed_dict={x:test_data_features})\n",
    "    result_label = [sum_to_pred(each) for each in result]\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    custom = mean_squared_error(result_label, y_test)\n",
    "    sess.close()\n",
    "    return sd,custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuning(5,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

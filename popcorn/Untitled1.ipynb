{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "# import tensorflow\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter\n",
    "from pipe import transform_text_func,FeatureExtractor, ImputeNA, CategoricalEncoding,text\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import RidgeClassifier,Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score, make_scorer,mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import csr_matrix\n",
    "import logging\n",
    "from sklearn.pipeline import make_pipeline, make_union \n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation, BatchNormalization, PReLU\n",
    "from keras.initializers import he_uniform\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "import gc\n",
    "# from gensim import corpora,models,similarities\n",
    "# import gensim\n",
    "\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------generate interaction feature between cate-------\n",
      "time elapsed:  41.4511284828186\n",
      "-------calculate uppercase prob to features-------\n",
      "time elapsed:  1.3654165267944336\n",
      "-------choose specific stop words-------\n",
      "total vocab:  45314\n",
      "vocab size frequency > 1:  45314\n",
      "time elapsed:  2.1865382194519043\n",
      "-------transform to features-------\n",
      "time elapsed:  68.8904595375061\n",
      "-------add length and upper prob to features-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (151824, 45760)\n",
      "time elapsed:  35.504518032073975\n",
      "--------------Word ridge data complete--------------\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('training_data.csv',header= 0 ,delimiter='\\t|\\n')\n",
    "print('-------generate interaction feature between cate-------')\n",
    "s = time.time()\n",
    "a = ['age_cat','sex','stay_cat','lang','er','category']\n",
    "\n",
    "for i,ai in enumerate(a):\n",
    "    for j,bj in enumerate(a):\n",
    "        if i<j:\n",
    "            x = train[ai]\n",
    "            y = train[bj]\n",
    "            t = []\n",
    "            for l in range(train.shape[0]):\n",
    "                t.append(str(x[l])+' '+ str(y[l]))\n",
    "            train[ai+'_'+bj] = t\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "x_total = list(train.comment)\n",
    "y_total = list(train.score)\n",
    "\n",
    "print('-------calculate uppercase prob to features-------')\n",
    "s = time.time()\n",
    "uppercase = []\n",
    "for i in x_total:\n",
    "    length = len(i.split())\n",
    "    tmp = []\n",
    "    for j in i:\n",
    "        if j.isupper():\n",
    "            tmp.append(j)\n",
    "    uppercase.append(len(tmp)/length)\n",
    "uppercase = np.array(uppercase).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_up = scaler.fit_transform(uppercase)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------choose specific stop words-------')\n",
    "s=time.time()\n",
    "x_lower = [sublist.lower() for sublist in x_total]\n",
    "# punctuation = string.punctuation.replace\n",
    "# regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "def tokenize(text):\n",
    "#     text = regex.sub(\" \", text) # remove punctuation\n",
    "    text = text.replace('.',' ')\n",
    "#     text = text.replace(',',' ')\n",
    "#     text = text.replace('&',' and ')\n",
    "#     text = text.replace('/',' or ')\n",
    "    text = text.split()\n",
    "    return(text)\n",
    "x_lower = [tokenize(i) for i in x_lower]\n",
    "x_unlist = []\n",
    "for i in x_lower:\n",
    "    x_unlist += i\n",
    "vocab_dic = Counter(x_unlist)\n",
    "stopwords_num = 250\n",
    "# print([i[0] for i in vocab_dic.most_common(stopwords_num)],'\\n\\n')\n",
    "print('total vocab: ',len(vocab_dic.most_common()))\n",
    "maxfeature = len([i[0] for i in vocab_dic.most_common() if i[1]>0])\n",
    "print('vocab size frequency > 1: ', maxfeature)\n",
    "\n",
    "stop = [i[0] for i in vocab_dic.most_common(stopwords_num)]\n",
    "from itertools import compress\n",
    "x_5level = [list(compress(x_lower, list(np.array(y_total)==i))) for i in np.unique(y_total)]\n",
    "\n",
    "x_5level_unlist = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "for i in range(len(x_5level)):\n",
    "    for j in x_5level[i]:\n",
    "        x_5level_unlist[i] += j\n",
    "multilevel_vocab = []\n",
    "for i in range(11):\n",
    "    multilevel_vocab.append(Counter(x_5level_unlist[i]))\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "multilevel_stop = defaultdict(list)\n",
    "for i in range(11):\n",
    "    tt = len(x_5level_unlist[i])\n",
    "    for j in stop:\n",
    "        multilevel_stop[j].append(multilevel_vocab[i][j]/tt)\n",
    "\n",
    "stop_var = [(key,np.std(value)*1000) for key,value in multilevel_stop.items() ]\n",
    "stop = [i[0] for i in stop_var if i[1]<0.1]\n",
    "\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        punctuation = string.punctuation.replace('#','')\n",
    "#         .replace('!','').replace('?','').replace(\"'\",'').replace(\"/\",'').replace(\"@\",'').replace('\"','')\n",
    "        regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        text = text.replace('#1','')\n",
    "        text = text.replace('#2','')\n",
    "        text = text.replace('#3','')\n",
    "        text = text.replace('#4','')\n",
    "        text = text.replace('#5','')\n",
    "        text = text.replace('#6','')\n",
    "        text = text.replace('#7','')\n",
    "        text = text.replace('#8','')\n",
    "        text = text.replace('#9','')\n",
    "#         text = text.replace('/w',' with ')\n",
    "#         text = text.replace('/',' ')\n",
    "#         text = text.replace('&',' and ')\n",
    "#         text = text.replace(\"/\",' or ')\n",
    "#         text = text.replace('\"\"',' sarcasm ')\n",
    "#         text = text.replace(\"'d\",' ')\n",
    "#         text = text.replace(\"'s\",' ')\n",
    "#         text = text.replace(\"'re\",' ')\n",
    "#         text = text.replace(\"'ll\",' ')\n",
    "#         text = text.replace(\"'ve\",' ')    \n",
    "\n",
    "#         text = text.replace('.',' ')\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        tokens = []\n",
    "        tokens_ = [s.split() for s in sent_tokenize(text)]\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent   \n",
    "        filtered_tokens = [ps.stem(w.lower()) for w in tokens]\n",
    "        return filtered_tokens\n",
    "    except TypeError as e: print(text,e)\n",
    "\n",
    "print('-------transform to features-------')\n",
    "s=time.time()\n",
    "onehot_list = ['age_cat', 'sex', 'stay_cat', 'lang', 'er','age_cat_sex', 'age_cat_stay_cat',\n",
    "       'age_cat_lang', 'age_cat_er', 'age_cat_category', 'sex_stay_cat',\n",
    "       'sex_lang', 'sex_er', 'sex_category', 'stay_cat_lang', 'stay_cat_er',\n",
    "       'stay_cat_category', 'lang_er', 'lang_category', 'er_category']\n",
    "onehot_pipeline = make_pipeline(FeatureExtractor(onehot_list),\n",
    "                                CategoricalEncoding('OneHot'),\n",
    "                                )\n",
    "descrip_pipeline = make_pipeline(FeatureExtractor('comment'),\n",
    "                                text(method='tfidf', ngram = 3, max_f = maxfeature, \n",
    "                                     binary = False, stopwords=stop,tokenizer=tokenize,analyzer ='word'))\n",
    "\n",
    "feature_union = make_union(\n",
    "    onehot_pipeline,\n",
    "    descrip_pipeline\n",
    ")\n",
    "X = feature_union.fit_transform(train)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------add length and upper prob to features-------')\n",
    "s = time.time()\n",
    "length = np.array([(X[i,]!=0).sum() for i in range(X.shape[0])]).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_l = scaler.fit_transform(length)\n",
    "X = hstack([X,new_l],format='csr')\n",
    "X = hstack([X,new_up],format='csr')\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "x_train,y_train = X, y_total\n",
    "\n",
    "print('--------------Word ridge data complete--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------generate interaction feature between cate-------\n",
      "time elapsed:  7.466782093048096\n",
      "-------calculate uppercase prob to features-------\n",
      "time elapsed:  0.23914670944213867\n",
      "-------choose specific stop words-------\n",
      "total vocab:  17572\n",
      "vocab size frequency > 1:  17572\n",
      "time elapsed:  0.3989870548248291\n",
      "-------transform to features-------\n",
      "time elapsed:  11.973384380340576\n",
      "-------add length and upper prob to features-------\n",
      "X shape:  (26954, 17997)\n",
      "time elapsed:  5.664021730422974\n",
      "--------------Word ridge data complete--------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('test_data.csv',header= 0 ,delimiter='\\t|\\n')\n",
    "print('-------generate interaction feature between cate-------')\n",
    "s = time.time()\n",
    "a = ['age_cat','sex','stay_cat','lang','er','category']\n",
    "\n",
    "for i,ai in enumerate(a):\n",
    "    for j,bj in enumerate(a):\n",
    "        if i<j:\n",
    "            x = train[ai]\n",
    "            y = train[bj]\n",
    "            t = []\n",
    "            for l in range(train.shape[0]):\n",
    "                t.append(str(x[l])+' '+ str(y[l]))\n",
    "            train[ai+'_'+bj] = t\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "x_total = list(train.comment)\n",
    "#y_total = list(train.score)\n",
    "\n",
    "print('-------calculate uppercase prob to features-------')\n",
    "s = time.time()\n",
    "uppercase = []\n",
    "for i in x_total:\n",
    "    length = len(i.split())\n",
    "    tmp = []\n",
    "    for j in i:\n",
    "        if j.isupper():\n",
    "            tmp.append(j)\n",
    "    uppercase.append(len(tmp)/length)\n",
    "uppercase = np.array(uppercase).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_up = scaler.fit_transform(uppercase)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------choose specific stop words-------')\n",
    "s=time.time()\n",
    "x_lower = [sublist.lower() for sublist in x_total]\n",
    "# punctuation = string.punctuation.replace\n",
    "# regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "def tokenize(text):\n",
    "#     text = regex.sub(\" \", text) # remove punctuation\n",
    "    text = text.replace('.',' ')\n",
    "#     text = text.replace(',',' ')\n",
    "#     text = text.replace('&',' and ')\n",
    "#     text = text.replace('/',' or ')\n",
    "    text = text.split()\n",
    "    return(text)\n",
    "x_lower = [tokenize(i) for i in x_lower]\n",
    "x_unlist = []\n",
    "for i in x_lower:\n",
    "    x_unlist += i\n",
    "vocab_dic = Counter(x_unlist)\n",
    "stopwords_num = 250\n",
    "# print([i[0] for i in vocab_dic.most_common(stopwords_num)],'\\n\\n')\n",
    "print('total vocab: ',len(vocab_dic.most_common()))\n",
    "maxfeature = len([i[0] for i in vocab_dic.most_common() if i[1]>0])\n",
    "print('vocab size frequency > 1: ', maxfeature)\n",
    "\n",
    "stop = [i[0] for i in vocab_dic.most_common(stopwords_num)]\n",
    "from itertools import compress\n",
    "x_5level = [list(compress(x_lower, list(np.array(y_total)==i))) for i in np.unique(y_total)]\n",
    "\n",
    "x_5level_unlist = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "for i in range(len(x_5level)):\n",
    "    for j in x_5level[i]:\n",
    "        x_5level_unlist[i] += j\n",
    "multilevel_vocab = []\n",
    "for i in range(11):\n",
    "    multilevel_vocab.append(Counter(x_5level_unlist[i]))\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "multilevel_stop = defaultdict(list)\n",
    "for i in range(11):\n",
    "    tt = len(x_5level_unlist[i])\n",
    "    for j in stop:\n",
    "        multilevel_stop[j].append(multilevel_vocab[i][j]/tt)\n",
    "\n",
    "stop_var = [(key,np.std(value)*1000) for key,value in multilevel_stop.items() ]\n",
    "stop = [i[0] for i in stop_var if i[1]<0.1]\n",
    "\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        punctuation = string.punctuation.replace('#','')\n",
    "#         .replace('!','').replace('?','').replace(\"'\",'').replace(\"/\",'').replace(\"@\",'').replace('\"','')\n",
    "        regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        text = text.replace('#1','')\n",
    "        text = text.replace('#2','')\n",
    "        text = text.replace('#3','')\n",
    "        text = text.replace('#4','')\n",
    "        text = text.replace('#5','')\n",
    "        text = text.replace('#6','')\n",
    "        text = text.replace('#7','')\n",
    "        text = text.replace('#8','')\n",
    "        text = text.replace('#9','')\n",
    "#         text = text.replace('/w',' with ')\n",
    "#         text = text.replace('/',' ')\n",
    "#         text = text.replace('&',' and ')\n",
    "#         text = text.replace(\"/\",' or ')\n",
    "#         text = text.replace('\"\"',' sarcasm ')\n",
    "#         text = text.replace(\"'d\",' ')\n",
    "#         text = text.replace(\"'s\",' ')\n",
    "#         text = text.replace(\"'re\",' ')\n",
    "#         text = text.replace(\"'ll\",' ')\n",
    "#         text = text.replace(\"'ve\",' ')    \n",
    "\n",
    "#         text = text.replace('.',' ')\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        tokens = []\n",
    "        tokens_ = [s.split() for s in sent_tokenize(text)]\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent   \n",
    "        filtered_tokens = [ps.stem(w.lower()) for w in tokens]\n",
    "        return filtered_tokens\n",
    "    except TypeError as e: print(text,e)\n",
    "\n",
    "print('-------transform to features-------')\n",
    "s=time.time()\n",
    "onehot_list = ['age_cat', 'sex', 'stay_cat', 'lang', 'er','age_cat_sex', 'age_cat_stay_cat',\n",
    "       'age_cat_lang', 'age_cat_er', 'age_cat_category', 'sex_stay_cat',\n",
    "       'sex_lang', 'sex_er', 'sex_category', 'stay_cat_lang', 'stay_cat_er',\n",
    "       'stay_cat_category', 'lang_er', 'lang_category', 'er_category']\n",
    "onehot_pipeline = make_pipeline(FeatureExtractor(onehot_list),\n",
    "                                CategoricalEncoding('OneHot'),\n",
    "                                )\n",
    "descrip_pipeline = make_pipeline(FeatureExtractor('comment'),\n",
    "                                text(method='tfidf', ngram = 3, max_f = maxfeature, \n",
    "                                     binary = False, stopwords=stop,tokenizer=tokenize,analyzer ='word'))\n",
    "\n",
    "feature_union = make_union(\n",
    "    onehot_pipeline,\n",
    "    descrip_pipeline\n",
    ")\n",
    "X = feature_union.fit_transform(train)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------add length and upper prob to features-------')\n",
    "s = time.time()\n",
    "length = np.array([(X[i,]!=0).sum() for i in range(X.shape[0])]).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_l = scaler.fit_transform(length)\n",
    "X = hstack([X,new_l],format='csr')\n",
    "X = hstack([X,new_up],format='csr')\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "x_test=X\n",
    "\n",
    "print('--------------Word ridge data complete--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------Ridge-------')\n",
    "s = time.time()\n",
    "# rkf = RepeatedKFold(n_splits=5, n_repeats=1)\n",
    "# parameters = {'alpha':[1.3,1.5,10]}\n",
    "# mse_score = make_scorer(mean_squared_error)\n",
    "# ridge_model = RidgeClassifier()\n",
    "# ridge_cv = GridSearchCV(ridge_model, parameters,cv=rkf, pre_dispatch=2, return_train_score = True,scoring=mse_score)\n",
    "# ridge_cv.fit(x_train, y_train)\n",
    "# print(ridge_cv.cv_results_)\n",
    "# ridge_train_res = ridge_cv.predict(x_train)\n",
    "# ridge_test_res = ridge_cv.predict(x_test)\n",
    "ridge_model = Ridge(alpha = 1.5)\n",
    "ridge_model = ridge_model.fit(x_train, y_train)\n",
    "ridge_train_res = ridge_model.predict(x_train)\n",
    "ridge_test_res = ridge_model.predict(x_test)\n",
    "joblib.dump(ridge_test_res, \"ridge_test_res.pkl\")\n",
    "\n",
    "\n",
    "ridge_train_res1 = [10 if i >10 else round(i) for i in ridge_train_res]\n",
    "ridge_train_res1 = np.array([0 if i<0 else i for i in ridge_train_res1])\n",
    "ridge_test_res1 = [10 if i >10 else round(i) for i in ridge_test_res]\n",
    "ridge_test_res1 = np.array([0 if i<0 else i for i in ridge_test_res1])\n",
    "\n",
    "print(\"train accuracy:\", mean_squared_error(ridge_train_res1, y_train))\n",
    "print(\"test accuracy:\", mean_squared_error(ridge_test_res1, y_test))\n",
    "print('time elapsed: ', time.time()-s)\n",
    "r_w = ridge_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dimension mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e549a07002cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrdinalRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sag\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/mord/regression_based.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRidge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_y_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \"\"\"\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0;32m--> 241\u001b[0;31m                                dense_output=True) + self.intercept_\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \"\"\"\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"toarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0;31m# dense row or column vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dimension mismatch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dimension mismatch"
     ]
    }
   ],
   "source": [
    "import mord\n",
    "classifier = mord.OrdinalRidge(max_iter=100000,solver=\"sag\",tol=0.001)\n",
    "classifier.fit(x_train,y_train)\n",
    "prediction = classifier.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((151824, 45760), (26954, 17997))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

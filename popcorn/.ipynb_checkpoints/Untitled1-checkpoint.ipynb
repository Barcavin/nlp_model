{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "# import tensorflow\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter\n",
    "from pipe import transform_text_func,FeatureExtractor, ImputeNA, CategoricalEncoding,text\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import RidgeClassifier,Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score, make_scorer,mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import csr_matrix\n",
    "import logging\n",
    "from sklearn.pipeline import make_pipeline, make_union \n",
    "from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation, BatchNormalization, PReLU\n",
    "from keras.initializers import he_uniform\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "import gc\n",
    "# from gensim import corpora,models,similarities\n",
    "# import gensim\n",
    "\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liukj\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------generate interaction feature between cate-------\n",
      "time elapsed:  42.836180686950684\n",
      "-------calculate uppercase prob to features-------\n",
      "time elapsed:  1.4722411632537842\n",
      "-------choose specific stop words-------\n",
      "total vocab:  45314\n",
      "vocab size frequency > 1:  45314\n",
      "time elapsed:  1.4910948276519775\n",
      "-------transform to features-------\n",
      "time elapsed:  67.09125709533691\n",
      "-------add length and upper prob to features-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liukj\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('training_data.csv',header= 0 ,delimiter='\\t|\\n')\n",
    "print('-------generate interaction feature between cate-------')\n",
    "s = time.time()\n",
    "a = ['age_cat','sex','stay_cat','lang','er','category']\n",
    "\n",
    "for i,ai in enumerate(a):\n",
    "    for j,bj in enumerate(a):\n",
    "        if i<j:\n",
    "            x = train[ai]\n",
    "            y = train[bj]\n",
    "            t = []\n",
    "            for l in range(train.shape[0]):\n",
    "                t.append(str(x[l])+' '+ str(y[l]))\n",
    "            train[ai+'_'+bj] = t\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "x_total = list(train.comment)\n",
    "y_total = list(train.score)\n",
    "\n",
    "print('-------calculate uppercase prob to features-------')\n",
    "s = time.time()\n",
    "uppercase = []\n",
    "for i in x_total:\n",
    "    length = len(i.split())\n",
    "    tmp = []\n",
    "    for j in i:\n",
    "        if j.isupper():\n",
    "            tmp.append(j)\n",
    "    uppercase.append(len(tmp)/length)\n",
    "uppercase = np.array(uppercase).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_up = scaler.fit_transform(uppercase)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------choose specific stop words-------')\n",
    "s=time.time()\n",
    "x_lower = [sublist.lower() for sublist in x_total]\n",
    "# punctuation = string.punctuation.replace\n",
    "# regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "def tokenize(text):\n",
    "#     text = regex.sub(\" \", text) # remove punctuation\n",
    "    text = text.replace('.',' ')\n",
    "#     text = text.replace(',',' ')\n",
    "#     text = text.replace('&',' and ')\n",
    "#     text = text.replace('/',' or ')\n",
    "    text = text.split()\n",
    "    return(text)\n",
    "x_lower = [tokenize(i) for i in x_lower]\n",
    "x_unlist = []\n",
    "for i in x_lower:\n",
    "    x_unlist += i\n",
    "vocab_dic = Counter(x_unlist)\n",
    "stopwords_num = 250\n",
    "# print([i[0] for i in vocab_dic.most_common(stopwords_num)],'\\n\\n')\n",
    "print('total vocab: ',len(vocab_dic.most_common()))\n",
    "maxfeature = len([i[0] for i in vocab_dic.most_common() if i[1]>0])\n",
    "print('vocab size frequency > 1: ', maxfeature)\n",
    "\n",
    "stop = [i[0] for i in vocab_dic.most_common(stopwords_num)]\n",
    "from itertools import compress\n",
    "x_5level = [list(compress(x_lower, list(np.array(y_total)==i))) for i in np.unique(y_total)]\n",
    "\n",
    "x_5level_unlist = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "for i in range(len(x_5level)):\n",
    "    for j in x_5level[i]:\n",
    "        x_5level_unlist[i] += j\n",
    "multilevel_vocab = []\n",
    "for i in range(11):\n",
    "    multilevel_vocab.append(Counter(x_5level_unlist[i]))\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "multilevel_stop = defaultdict(list)\n",
    "for i in range(11):\n",
    "    tt = len(x_5level_unlist[i])\n",
    "    for j in stop:\n",
    "        multilevel_stop[j].append(multilevel_vocab[i][j]/tt)\n",
    "\n",
    "stop_var = [(key,np.std(value)*1000) for key,value in multilevel_stop.items() ]\n",
    "stop = [i[0] for i in stop_var if i[1]<0.1]\n",
    "\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        punctuation = string.punctuation.replace('#','')\n",
    "#         .replace('!','').replace('?','').replace(\"'\",'').replace(\"/\",'').replace(\"@\",'').replace('\"','')\n",
    "        regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        text = text.replace('#1','')\n",
    "        text = text.replace('#2','')\n",
    "        text = text.replace('#3','')\n",
    "        text = text.replace('#4','')\n",
    "        text = text.replace('#5','')\n",
    "        text = text.replace('#6','')\n",
    "        text = text.replace('#7','')\n",
    "        text = text.replace('#8','')\n",
    "        text = text.replace('#9','')\n",
    "#         text = text.replace('/w',' with ')\n",
    "#         text = text.replace('/',' ')\n",
    "#         text = text.replace('&',' and ')\n",
    "#         text = text.replace(\"/\",' or ')\n",
    "#         text = text.replace('\"\"',' sarcasm ')\n",
    "#         text = text.replace(\"'d\",' ')\n",
    "#         text = text.replace(\"'s\",' ')\n",
    "#         text = text.replace(\"'re\",' ')\n",
    "#         text = text.replace(\"'ll\",' ')\n",
    "#         text = text.replace(\"'ve\",' ')    \n",
    "\n",
    "#         text = text.replace('.',' ')\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        tokens = []\n",
    "        tokens_ = [s.split() for s in sent_tokenize(text)]\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent   \n",
    "        filtered_tokens = [ps.stem(w.lower()) for w in tokens]\n",
    "        return filtered_tokens\n",
    "    except TypeError as e: print(text,e)\n",
    "\n",
    "print('-------transform to features-------')\n",
    "s=time.time()\n",
    "onehot_list = ['age_cat', 'sex', 'stay_cat', 'lang', 'er','age_cat_sex', 'age_cat_stay_cat',\n",
    "       'age_cat_lang', 'age_cat_er', 'age_cat_category', 'sex_stay_cat',\n",
    "       'sex_lang', 'sex_er', 'sex_category', 'stay_cat_lang', 'stay_cat_er',\n",
    "       'stay_cat_category', 'lang_er', 'lang_category', 'er_category']\n",
    "onehot_pipeline = make_pipeline(FeatureExtractor(onehot_list),\n",
    "                                CategoricalEncoding('OneHot'),\n",
    "                                )\n",
    "descrip_pipeline = make_pipeline(FeatureExtractor('comment'),\n",
    "                                text(method='tfidf', ngram = 3, max_f = maxfeature, \n",
    "                                     binary = False, stopwords=stop,tokenizer=tokenize,analyzer ='word'))\n",
    "\n",
    "feature_union = make_union(\n",
    "    onehot_pipeline,\n",
    "    descrip_pipeline\n",
    ")\n",
    "X = feature_union.fit_transform(train)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------add length and upper prob to features-------')\n",
    "s = time.time()\n",
    "length = np.array([(X[i,]!=0).sum() for i in range(X.shape[0])]).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_l = scaler.fit_transform(length)\n",
    "X = hstack([X,new_l],format='csr')\n",
    "X = hstack([X,new_up],format='csr')\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y_total, test_size=0.33, random_state=109)\n",
    "\n",
    "print('--------------Word ridge data complete--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y_total, test_size=0.33, random_state=311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------Ridge-------')\n",
    "s = time.time()\n",
    "# rkf = RepeatedKFold(n_splits=5, n_repeats=1)\n",
    "# parameters = {'alpha':[1.3,1.5,10]}\n",
    "# mse_score = make_scorer(mean_squared_error)\n",
    "# ridge_model = RidgeClassifier()\n",
    "# ridge_cv = GridSearchCV(ridge_model, parameters,cv=rkf, pre_dispatch=2, return_train_score = True,scoring=mse_score)\n",
    "# ridge_cv.fit(x_train, y_train)\n",
    "# print(ridge_cv.cv_results_)\n",
    "# ridge_train_res = ridge_cv.predict(x_train)\n",
    "# ridge_test_res = ridge_cv.predict(x_test)\n",
    "ridge_model = Ridge(alpha = 1.5)\n",
    "ridge_model = ridge_model.fit(x_train, y_train)\n",
    "ridge_train_res = ridge_model.predict(x_train)\n",
    "ridge_test_res = ridge_model.predict(x_test)\n",
    "joblib.dump(ridge_test_res, \"ridge_test_res.pkl\")\n",
    "\n",
    "\n",
    "ridge_train_res1 = [10 if i >10 else round(i) for i in ridge_train_res]\n",
    "ridge_train_res1 = np.array([0 if i<0 else i for i in ridge_train_res1])\n",
    "ridge_test_res1 = [10 if i >10 else round(i) for i in ridge_test_res]\n",
    "ridge_test_res1 = np.array([0 if i<0 else i for i in ridge_test_res1])\n",
    "\n",
    "print(\"train accuracy:\", mean_squared_error(ridge_train_res1, y_train))\n",
    "print(\"test accuracy:\", mean_squared_error(ridge_test_res1, y_test))\n",
    "print('time elapsed: ', time.time()-s)\n",
    "r_w = ridge_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('training_data.csv',header= 0 ,delimiter='\\t|\\n')\n",
    "print('-------generate interaction feature between cate-------')\n",
    "s = time.time()\n",
    "a = ['age_cat','sex','stay_cat','lang','er','category']\n",
    "\n",
    "for i,ai in enumerate(a):\n",
    "    for j,bj in enumerate(a):\n",
    "        if i<j:\n",
    "            x = train[ai]\n",
    "            y = train[bj]\n",
    "            t = []\n",
    "            for l in range(train.shape[0]):\n",
    "                t.append(str(x[l])+' '+ str(y[l]))\n",
    "            train[ai+'_'+bj] = t\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "x_total = list(train.comment)\n",
    "y_total = list(train.score)\n",
    "\n",
    "print('-------calculate uppercase prob to features-------')\n",
    "s = time.time()\n",
    "uppercase = []\n",
    "for i in x_total:\n",
    "    length = len(i.split())\n",
    "    tmp = []\n",
    "    for j in i:\n",
    "        if j.isupper():\n",
    "            tmp.append(j)\n",
    "    uppercase.append(len(tmp)/length)\n",
    "uppercase = np.array(uppercase).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_up = scaler.fit_transform(uppercase)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------choose specific stop words-------')\n",
    "s=time.time()\n",
    "x_lower = [sublist.lower() for sublist in x_total]\n",
    "# punctuation = string.punctuation.replace\n",
    "# regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "def tokenize(text):\n",
    "#     text = regex.sub(\" \", text) # remove punctuation\n",
    "    text = text.replace('.',' ')\n",
    "#     text = text.replace(',',' ')\n",
    "#     text = text.replace('&',' and ')\n",
    "#     text = text.replace('/',' or ')\n",
    "    text = text.split()\n",
    "    return(text)\n",
    "x_lower = [tokenize(i) for i in x_lower]\n",
    "x_unlist = []\n",
    "for i in x_lower:\n",
    "    x_unlist += i\n",
    "vocab_dic = Counter(x_unlist)\n",
    "stopwords_num = 250\n",
    "# print([i[0] for i in vocab_dic.most_common(stopwords_num)],'\\n\\n')\n",
    "print('total vocab: ',len(vocab_dic.most_common()))\n",
    "maxfeature = len([i[0] for i in vocab_dic.most_common() if i[1]>0])\n",
    "print('vocab size frequency > 1: ', maxfeature)\n",
    "\n",
    "stop = [i[0] for i in vocab_dic.most_common(stopwords_num)]\n",
    "from itertools import compress\n",
    "x_5level = [list(compress(x_lower, list(np.array(y_total)==i))) for i in np.unique(y_total)]\n",
    "\n",
    "x_5level_unlist = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "for i in range(len(x_5level)):\n",
    "    for j in x_5level[i]:\n",
    "        x_5level_unlist[i] += j\n",
    "multilevel_vocab = []\n",
    "for i in range(11):\n",
    "    multilevel_vocab.append(Counter(x_5level_unlist[i]))\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "multilevel_stop = defaultdict(list)\n",
    "for i in range(11):\n",
    "    tt = len(x_5level_unlist[i])\n",
    "    for j in stop:\n",
    "        multilevel_stop[j].append(multilevel_vocab[i][j]/tt)\n",
    "\n",
    "stop_var = [(key,np.std(value)*1000) for key,value in multilevel_stop.items() ]\n",
    "stop = [i[0] for i in stop_var if i[1]<0.1]\n",
    "\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        punctuation = string.punctuation.replace('#','')\n",
    "#         .replace('!','').replace('?','').replace(\"'\",'').replace(\"/\",'').replace(\"@\",'').replace('\"','')\n",
    "        regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        text = text.replace('#1','')\n",
    "        text = text.replace('#2','')\n",
    "        text = text.replace('#3','')\n",
    "        text = text.replace('#4','')\n",
    "        text = text.replace('#5','')\n",
    "        text = text.replace('#6','')\n",
    "        text = text.replace('#7','')\n",
    "        text = text.replace('#8','')\n",
    "        text = text.replace('#9','')\n",
    "#         text = text.replace('/w',' with ')\n",
    "#         text = text.replace('/',' ')\n",
    "#         text = text.replace('&',' and ')\n",
    "#         text = text.replace(\"/\",' or ')\n",
    "#         text = text.replace('\"\"',' sarcasm ')\n",
    "#         text = text.replace(\"'d\",' ')\n",
    "#         text = text.replace(\"'s\",' ')\n",
    "#         text = text.replace(\"'re\",' ')\n",
    "#         text = text.replace(\"'ll\",' ')\n",
    "#         text = text.replace(\"'ve\",' ')    \n",
    "\n",
    "#         text = text.replace('.',' ')\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        tokens = []\n",
    "        tokens_ = [s.split() for s in sent_tokenize(text)]\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent   \n",
    "        filtered_tokens = [ps.stem(w.lower()) for w in tokens]\n",
    "        return filtered_tokens\n",
    "    except TypeError as e: print(text,e)\n",
    "\n",
    "print('-------transform to features-------')\n",
    "s=time.time()\n",
    "onehot_list = ['age_cat', 'sex', 'stay_cat', 'lang', 'er','age_cat_sex', 'age_cat_stay_cat',\n",
    "       'age_cat_lang', 'age_cat_er', 'age_cat_category', 'sex_stay_cat',\n",
    "       'sex_lang', 'sex_er', 'sex_category', 'stay_cat_lang', 'stay_cat_er',\n",
    "       'stay_cat_category', 'lang_er', 'lang_category', 'er_category']\n",
    "onehot_pipeline = make_pipeline(FeatureExtractor(onehot_list),\n",
    "                                CategoricalEncoding('OneHot'),\n",
    "                                )\n",
    "descrip_pipeline = make_pipeline(FeatureExtractor('comment'),\n",
    "                                text(method='tfidf', ngram = 3, max_f = maxfeature, \n",
    "                                     binary = False, stopwords=stop,tokenizer=tokenize,analyzer ='char'))\n",
    "\n",
    "feature_union = make_union(\n",
    "    onehot_pipeline,\n",
    "    descrip_pipeline\n",
    ")\n",
    "X = feature_union.fit_transform(train)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------add length and upper prob to features-------')\n",
    "s = time.time()\n",
    "length = np.array([(X[i,]!=0).sum() for i in range(X.shape[0])]).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_l = scaler.fit_transform(length)\n",
    "X = hstack([X,new_l],format='csr')\n",
    "X = hstack([X,new_up],format='csr')\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y_total, test_size=0.33, random_state=109)\n",
    "\n",
    "print('--------------char ridge data complete--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y_total, test_size=0.33, random_state=311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('-------Ridge-------')\n",
    "s = time.time()\n",
    "# rkf = RepeatedKFold(n_splits=5, n_repeats=1)\n",
    "# parameters = {'alpha':[1.3,1.5,10]}\n",
    "# mse_score = make_scorer(mean_squared_error)\n",
    "# ridge_model = RidgeClassifier()\n",
    "# ridge_cv = GridSearchCV(ridge_model, parameters,cv=rkf, pre_dispatch=2, return_train_score = True,scoring=mse_score)\n",
    "# ridge_cv.fit(x_train, y_train)\n",
    "# print(ridge_cv.cv_results_)\n",
    "# ridge_train_res = ridge_cv.predict(x_train)\n",
    "# ridge_test_res = ridge_cv.predict(x_test)\n",
    "ridge_model = Ridge(alpha = 1.5)\n",
    "ridge_model = ridge_model.fit(x_train, y_train)\n",
    "ridge_train_res = ridge_model.predict(x_train)\n",
    "ridge_test_res = ridge_model.predict(x_test)\n",
    "joblib.dump(ridge_test_res, \"ridge_test_res.pkl\")\n",
    "\n",
    "\n",
    "ridge_train_res1 = [10 if i >10 else round(i) for i in ridge_train_res]\n",
    "ridge_train_res1 = np.array([0 if i<0 else i for i in ridge_train_res1])\n",
    "ridge_test_res1 = [10 if i >10 else round(i) for i in ridge_test_res]\n",
    "ridge_test_res1 = np.array([0 if i<0 else i for i in ridge_test_res1])\n",
    "\n",
    "print(\"train accuracy:\", mean_squared_error(ridge_train_res1, y_train))\n",
    "print(\"test accuracy:\", mean_squared_error(ridge_test_res1, y_test))\n",
    "print('time elapsed: ', time.time()-s)\n",
    "r_c = ridge_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = 0.2*r+0.8*ridge_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [10 if i >10 else round(i) for i in result]\n",
    "result = [0 if i<0 else round(i) for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test accuracy:\", mean_squared_error(result, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('-------SparseNN-------')\n",
    "# s = time.time()\n",
    "\n",
    "# def sparseNN():                                             \n",
    "#     sparse_data = Input(shape=[x_train.shape[1]], dtype = 'float32', sparse = True, name='sparse_data')  \n",
    "\n",
    "#     x = Dense(200 , kernel_initializer=he_uniform(seed=0) )(sparse_data)    \n",
    "#     x = PReLU()(x)\n",
    "#     x = Dense(150 , kernel_initializer=he_uniform(seed=0) )(x)\n",
    "#     x = PReLU()(x)\n",
    "#     x = Dense(100, kernel_initializer=he_uniform(seed=0) )(x)\n",
    "#     x = PReLU()(x)\n",
    "#     x= Dense(1)(x)\n",
    "    \n",
    "#     model = Model([sparse_data],x)\n",
    "    \n",
    "#     optimizer = Adam(.001)\n",
    "#     model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "#     return model\n",
    "\n",
    "# BATCH_SIZE = 1000\n",
    "# epochs = 20\n",
    "\n",
    "# sparse_nn = sparseNN()\n",
    "\n",
    "# print(\"Fitting SPARSE NN model ...\")\n",
    "\n",
    "# for ep in range(epochs):\n",
    "#     BATCH_SIZE = int(BATCH_SIZE*2)\n",
    "#     sparse_nn.fit(x_train, np.array(y_train).reshape(-1,1), \n",
    "#                       batch_size=BATCH_SIZE, epochs=1, verbose=10 )\n",
    "\n",
    "# gc.collect\n",
    "# nn_test_res = sparse_nn.predict(x_test, batch_size=len(y_test))\n",
    "# nn_train_res = sparse_nn.predict(x_train, batch_size=len(y_train))\n",
    "# joblib.dump(nn_test_res, \"nn_test_res.pkl\")\n",
    "\n",
    "\n",
    "# nn_train_res = [i[0] for i in nn_train_res] \n",
    "# nn_train_res = [10 if i >10 else round(i) for i in nn_train_res]\n",
    "# nn_train_res = [0 if i<0 else round(i) for i in nn_train_res]\n",
    "# print(\"train accuracy:\", mean_squared_error(nn_train_res, y_train))\n",
    "# nn_test_res = [i[0] for i in nn_test_res] \n",
    "# # nn_test_res = [10 if i >10 else round(i) for i in nn_test_res]\n",
    "# # nn_test_res = [0 if i<0 else round(i) for i in nn_test_res]\n",
    "# print(\"test accuracy:\", mean_squared_error(nn_test_res, y_test))\n",
    "# print('time elapsed: ', time.time()-s)\n",
    "# print('-------SGD-------')\n",
    "# s = time.time()\n",
    "\n",
    "# from sklearn.linear_model import SGDRegressor\n",
    "# import sklearn\n",
    "# model_sgd=SGDRegressor(loss = \"squared_epsilon_insensitive\",\\\n",
    "#                          n_iter = 600,\\\n",
    "#                            penalty=\"l2\",\\\n",
    "#                            alpha=0.00000001,\\\n",
    "# #                            penalty : str, ‘none’, ‘l2’, ‘l1’, or ‘elasticnet’\n",
    "#                          epsilon=0.0001,\\\n",
    "#                          random_state = 30,\\\n",
    "#                          shuffle = True)\n",
    "# model_sgd = model_sgd.fit(x_train,y_train)\n",
    "# sgd_train_res = model_sgd.predict(x_train)\n",
    "# sgd_train_res = [10 if i >10 else round(i) for i in sgd_train_res]\n",
    "# sgd_train_res = [0 if i<0 else round(i) for i in sgd_train_res]\n",
    "# print(\"train accuracy:\", mean_squared_error(sgd_train_res, y_train))\n",
    "# sgd_test_res = model_sgd.predict(x_test)\n",
    "# joblib.dump(sgd_test_res, \"sgd_test_res.pkl\")\n",
    "\n",
    "# # sgd_test_res = [10 if i >10 else round(i) for i in sgd_test_res]\n",
    "# # sgd_test_res = [0 if i<0 else round(i) for i in sgd_test_res]\n",
    "# print(\"test accuracy:\", mean_squared_error(sgd_test_res, y_test))\n",
    "# print('time elapsed: ', time.time()-s)\n",
    "\n",
    "# print('-------SVM-------')\n",
    "# s = time.time()\n",
    "# model_svm=sklearn.svm.LinearSVR(epsilon=0,\\\n",
    "#                                     tol=0.000001,\\\n",
    "#                                     C=0.1, \\\n",
    "#                                     loss=\"squared_epsilon_insensitive\", \\\n",
    "#                                     fit_intercept=True, \\\n",
    "#                                     intercept_scaling=1.0, \\\n",
    "#                                     dual=True, verbose=0, \\\n",
    "#                                     random_state=30, \\\n",
    "#                                     max_iter=10000)\n",
    "\n",
    "# model_svm = model_svm.fit(x_train,y_train)\n",
    "# svm_train_res = model_svm.predict(x_train)\n",
    "# # svm_train_res = [10 if i >10 else round(i) for i in svm_train_res]\n",
    "# # svm_train_res = [0 if i<0 else round(i) for i in svm_train_res]\n",
    "# print(\"train accuracy:\", mean_squared_error(svm_train_res, y_train))\n",
    "\n",
    "# svm_test_res = model_svm.predict(x_test)\n",
    "# joblib.dump(svm_test_res, \"svm_test_res.pkl\")\n",
    "\n",
    "# # svm_test_res = [10 if i >10 else round(i) for i in svm_test_res]\n",
    "# # svm_test_res = [0 if i<0 else round(i) for i in svm_test_res]\n",
    "# print(\"test accuracy:\", mean_squared_error(svm_test_res, y_test))\n",
    "# print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_test_res1 = [10 if i >10 else round(i) for i in ridge_test_res]\n",
    "ridge_test_res1 = np.array([0 if i<0 else i for i in ridge_test_res1])\n",
    "print(\"test accuracy:\", mean_squared_error(ridge_test_res1, y_test))\n",
    "# nn_test_res1 = [10 if i >10 else round(i) for i in nn_test_res]\n",
    "# nn_test_res1 = [0 if i<0 else round(i) for i in nn_test_res1]\n",
    "# print(\"test accuracy:\", mean_squared_error(nn_test_res1, y_test))\n",
    "# sgd_test_res1 = [10 if i >10 else round(i) for i in sgd_test_res]\n",
    "# sgd_test_res1 = [0 if i<0 else round(i) for i in sgd_test_res1]\n",
    "# print(\"test accuracy:\", mean_squared_error(sgd_test_res1, y_test))\n",
    "# svm_test_res1 = [10 if i >10 else round(i) for i in svm_test_res]\n",
    "# svm_test_res1 = [0 if i<0 else round(i) for i in svm_test_res1]\n",
    "# print(\"test accuracy:\", mean_squared_error(svm_test_res1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = ridge_test_res\n",
    "s = sgd_test_res\n",
    "v = svm_test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import tensorflow\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import Counter\n",
    "from pipe import transform_text_func,FeatureExtractor, ImputeNA, CategoricalEncoding,text\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, make_scorer,mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import PorterStemmer\n",
    "from scipy.sparse import csr_matrix\n",
    "import logging\n",
    "from sklearn.pipeline import make_pipeline, make_union \n",
    "#from keras.layers import Input, Dropout, Dense, concatenate, GRU, Embedding, Flatten, Activation, BatchNormalization, PReLU\n",
    "#from keras.initializers import he_uniform\n",
    "#from keras.layers import Conv1D\n",
    "#from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "#from keras.optimizers import Adam, SGD\n",
    "#from keras.models import Model\n",
    "#import gc\n",
    "# from gensim import corpora,models,similarities\n",
    "# import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('training_data.csv',header= 0 ,delimiter='\\t|\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------generate interaction feature between cate-------\n",
      "time elapsed:  38.513720750808716\n"
     ]
    }
   ],
   "source": [
    "print('-------generate interaction feature between cate-------')\n",
    "s = time.time()\n",
    "a = ['age_cat','sex','stay_cat','lang','er','category']\n",
    "\n",
    "for i,ai in enumerate(a):\n",
    "    for j,bj in enumerate(a):\n",
    "        if i<j:\n",
    "            x = train[ai]\n",
    "            y = train[bj]\n",
    "            t = []\n",
    "            for l in range(train.shape[0]):\n",
    "                t.append(str(x[l])+' '+ str(y[l]))\n",
    "            train[ai+'_'+bj] = t\n",
    "print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total = list(train.comment)\n",
    "y_total = list(train.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------calculate uppercase prob to features-------\n",
      "time elapsed:  1.370927095413208\n",
      "-------choose specific stop words-------\n",
      "total vocab:  44702\n",
      "vocab size frequency > 1:  20256\n",
      "time elapsed:  1.6601028442382812\n"
     ]
    }
   ],
   "source": [
    "print('-------calculate uppercase prob to features-------')\n",
    "s = time.time()\n",
    "uppercase = []\n",
    "for i in x_total:\n",
    "    length = len(i.split())\n",
    "    tmp = []\n",
    "    for j in i:\n",
    "        if j.isupper():\n",
    "            tmp.append(j)\n",
    "    uppercase.append(len(tmp)/length)\n",
    "uppercase = np.array(uppercase).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_up = scaler.fit_transform(uppercase)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------choose specific stop words-------')\n",
    "s=time.time()\n",
    "x_lower = [sublist.lower() for sublist in x_total]\n",
    "# punctuation = string.punctuation.replace('!','').replace('?','').replace(\"'\",'').replace('~','')\n",
    "# regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "def tokenize(text):\n",
    "#     text = regex.sub(\" \", text) # remove punctuation\n",
    "    text = text.replace('w/','with ')\n",
    "#     text = text.replace('&',' and ')\n",
    "#     text = text.replace('/',' or ')\n",
    "    text = text.replace('.',' ')\n",
    "    text = text.split()\n",
    "    return(text)\n",
    "x_lower = [tokenize(i) for i in x_lower]\n",
    "x_unlist = []\n",
    "for i in x_lower:\n",
    "    x_unlist += i\n",
    "vocab_dic = Counter(x_unlist)\n",
    "stopwords_num = 250\n",
    "# print([i[0] for i in vocab_dic.most_common(stopwords_num)],'\\n\\n')\n",
    "print('total vocab: ',len(vocab_dic.most_common()))\n",
    "maxfeature = len([i[0] for i in vocab_dic.most_common() if i[1]>1])\n",
    "print('vocab size frequency > 1: ', maxfeature)\n",
    "\n",
    "stop = [i[0] for i in vocab_dic.most_common(stopwords_num)]\n",
    "from itertools import compress\n",
    "x_5level = [list(compress(x_lower, list(np.array(y_total)==i))) for i in np.unique(y_total)]\n",
    "\n",
    "x_5level_unlist = [[],[],[],[],[],[],[],[],[],[],[]]\n",
    "for i in range(len(x_5level)):\n",
    "    for j in x_5level[i]:\n",
    "        x_5level_unlist[i] += j\n",
    "multilevel_vocab = []\n",
    "for i in range(11):\n",
    "    multilevel_vocab.append(Counter(x_5level_unlist[i]))\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "multilevel_stop = defaultdict(list)\n",
    "for i in range(11):\n",
    "    tt = len(x_5level_unlist[i])\n",
    "    for j in stop:\n",
    "        multilevel_stop[j].append(multilevel_vocab[i][j]/tt)\n",
    "\n",
    "stop_var = [(key,np.std(value)*1000) for key,value in multilevel_stop.items() ]\n",
    "stop = [i[0] for i in stop_var if i[1]<0.5]\n",
    "\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    try:\n",
    "        punctuation = string.punctuation.replace('!','').replace('?','').replace(\"'\",'').replace(\"/\",'').replace(\"@\",'').replace('\"','')\n",
    "        regex = re.compile('[' +re.escape(punctuation) +']')\n",
    "        text = regex.sub(\" \", text) # remove punctuation\n",
    "        text = text.replace('w/',' with ')\n",
    "        text = text.replace('&',' and ')\n",
    "        text = text.replace(\"/\",' or ')\n",
    "        text = text.replace('\"\"',' sarcasm ')\n",
    "        text = text.replace(\"'d\",' ')\n",
    "        text = text.replace(\"'s\",' ')\n",
    "        text = text.replace(\"'re\",' ')\n",
    "        text = text.replace(\"'ll\",' ')\n",
    "        text = text.replace(\"'ve\",' ')    \n",
    "#         text = text.replace('.0','')\n",
    "        text = text.replace('.',' ')\n",
    "\n",
    "        ps = PorterStemmer()\n",
    "        tokens = []\n",
    "        tokens_ = [s.split() for s in sent_tokenize(text)]\n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent   \n",
    "        filtered_tokens = [ps.stem(w.lower()) for w in tokens]\n",
    "        return filtered_tokens\n",
    "    except TypeError as e: print(text,e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------transform to features-------\n",
      "time elapsed:  61.86040806770325\n",
      "-------add length and upper prob to features-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (151824, 20702)\n",
      "time elapsed:  30.15910577774048\n"
     ]
    }
   ],
   "source": [
    "print('-------transform to features-------')\n",
    "s=time.time()\n",
    "onehot_list = ['age_cat', 'sex', 'stay_cat', 'lang', 'er','age_cat_sex', 'age_cat_stay_cat',\n",
    "       'age_cat_lang', 'age_cat_er', 'age_cat_category', 'sex_stay_cat',\n",
    "       'sex_lang', 'sex_er', 'sex_category', 'stay_cat_lang', 'stay_cat_er',\n",
    "       'stay_cat_category', 'lang_er', 'lang_category', 'er_category']\n",
    "onehot_pipeline = make_pipeline(FeatureExtractor(onehot_list),\n",
    "                                CategoricalEncoding('OneHot'),\n",
    "                                )\n",
    "descrip_pipeline = make_pipeline(FeatureExtractor('comment'),\n",
    "                                text(method='tfidf', ngram = 3, max_f = maxfeature, \n",
    "                                     binary = True, stopwords=stop,tokenizer=tokenize,analyzer ='word'))\n",
    "\n",
    "feature_union = make_union(\n",
    "    onehot_pipeline,\n",
    "    descrip_pipeline\n",
    ")\n",
    "X = feature_union.fit_transform(train)\n",
    "print('time elapsed: ', time.time()-s)\n",
    "\n",
    "print('-------add length and upper prob to features-------')\n",
    "s = time.time()\n",
    "length = np.array([(X[i,]!=0).sum() for i in range(X.shape[0])]).reshape(-1,1)\n",
    "scaler = StandardScaler()\n",
    "new_l = scaler.fit_transform(length)\n",
    "X = hstack([X,new_l],format='csr')\n",
    "X = hstack([X,new_up],format='csr')\n",
    "\n",
    "print('X shape: ',X.shape)\n",
    "print('time elapsed: ', time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((151824, 20702), 151824)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,len(y_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def int2vector(i):\n",
    "    return np.array([1]*(i+1)+[0]*(10-i)).reshape(11,1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_total, test_size=0.33, random_state=42)\n",
    "\n",
    "y_train = [int2vector(i) for i in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [X_train[i,] for i in range(X_train.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Reshaping not implemented for csr_matrix.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(self, shape, order)\u001b[0m\n\u001b[1;32m    128\u001b[0m         raise NotImplementedError(\"Reshaping not implemented for %s.\" %\n\u001b[0;32m--> 129\u001b[0;31m                                   self.__class__.__name__)\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Reshaping not implemented for csr_matrix.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-dfc1104a88f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20702\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Read matrix dimensions given, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m   \u001b[0;31m# spmatrix will check for errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36mset_shape\u001b[0;34m(self, shape)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 raise NotImplementedError(\"Reshaping not implemented for %s.\" %\n\u001b[0;32m---> 98\u001b[0;31m                                           self.__class__.__name__)\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Reshaping not implemented for csr_matrix."
     ]
    }
   ],
   "source": [
    "csr_matrix(train_data[0],shape=(20702,1),copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevindong1994/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'core' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-fd10f9dd8235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mpython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m# pylint: enable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'core' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "def to_predict(row):\n",
    "        for i in range(11):\n",
    "            if row[-1]>0.5:\n",
    "                return 10\n",
    "            elif row[i]<0.5:\n",
    "                return i-1\n",
    "\n",
    "logging = {\"i\":0,\"epoch\":0,\"epochs\":5}            \n",
    "            \n",
    "def generate_batch(features,labels,ids):\n",
    "    global logging\n",
    "    if logging[\"i\"]==len(ids)-1:\n",
    "        inner_i = logging[\"i\"]\n",
    "        logging[\"i\"] = 0\n",
    "        logging[\"epoch\"] += 1\n",
    "        return features[ids[inner_i]:],labels[ids[inner_i]:]\n",
    "    else:\n",
    "        inner_i = logging[\"i\"]\n",
    "        logging[\"i\"] += 1\n",
    "        return features[ids[inner_i]:ids[logging[\"i\"]]],labels[ids[inner_i]:ids[logging[\"i\"]]]\n",
    "\n",
    "\n",
    "\n",
    "def sum_to_pred(row):\n",
    "    r = np.rint(np.sum(row))\n",
    "    if r>10:\n",
    "        return 10\n",
    "    elif r<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "train_data = [X_train[i,] for i in range(X_train.shape[0])]\n",
    "#train_data = [np.array(each.todense())[0] for each in train_data]\n",
    "test_data = X_test\n",
    "test_data_features = [X_test[i,] for i in range(X_test.shape[0])]\n",
    "#test_data_features = [np.array(each.todense())[0] for each in test_data_features]\n",
    "\n",
    "batch_size = 50\n",
    "num_features=X_train.shape[1]\n",
    "H = 5\n",
    "\n",
    "logging[\"i\"] = 0\n",
    "logging[\"epoch\"] = 0\n",
    "ids = list(range(0,X_train.shape[0],batch_size))\n",
    "print(\"feature:%i,H:%i,batch:%i\"%(num_features,H,batch_size))\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32,shape=[None,num_features])\n",
    "    y_ = tf.placeholder(tf.float32,shape=[None,11,1])\n",
    "    new_y = tf.squeeze(y_)\n",
    "    W_1 = tf.Variable(tf.random_uniform([num_features,H]))\n",
    "    b_1 = tf.Variable(tf.zeros([H]))\n",
    "\n",
    "    W_2 = tf.Variable(tf.random_uniform([H,11]))\n",
    "    b_2 = tf.Variable(tf.zeros([11]))\n",
    "\n",
    "    temp_1 = tf.nn.relu(tf.matmul(x,W_1) + b_1)\n",
    "    temp_2 = tf.matmul(temp_1,W_2) + b_2\n",
    "    output = tf.nn.sigmoid(temp_2)\n",
    "    #loss = tf.reduce_mean(tf.square(tf.subtract(output,new_y)))\n",
    "    loss = tf.losses.mean_squared_error(output,new_y)\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    opt_op = opt.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "# with tf.Session(graph = graph) as session:\n",
    "\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "sess.run(init)\n",
    "print(\"Initialized\")\n",
    "step = 0 \n",
    "while logging[\"epoch\"]<logging[\"epochs\"]:\n",
    "    step+=1\n",
    "    batch_x,batch_y = generate_batch(train_data,y_train,ids)\n",
    "    feed_dict={x:batch_x,y_:batch_y}\n",
    "    _,loss_val = sess.run([opt_op,loss],feed_dict=feed_dict)\n",
    "    if step % 2000 == 0:\n",
    "        print(\"Step: %i, Epoch: %i, Loss:%f\"%(step,logging[\"epoch\"],loss_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_features[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sess.run(output,feed_dict={x:test_data_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sess.run(output,feed_dict={x:test_data_features})\n",
    "result_label = [to_predict(each) for each in result]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "sd = mean_squared_error(result_label, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = sess.run(output,feed_dict={x:test_data_features})\n",
    "result_label = [sum_to_pred(each) for each in result]\n",
    "from sklearn.metrics import mean_squared_error\n",
    "custom = mean_squared_error(result_label, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "def to_predict(row):\n",
    "        for i in range(11):\n",
    "            if row[-1]>0.5:\n",
    "                return 10\n",
    "            elif row[i]<0.5:\n",
    "                return i-1\n",
    "\n",
    "logging = {\"i\":0,\"epoch\":0,\"epochs\":20}            \n",
    "            \n",
    "def generate_batch(features,labels,ids):\n",
    "    global logging\n",
    "    if logging[\"i\"]==len(ids)-1:\n",
    "        inner_i = logging[\"i\"]\n",
    "        logging[\"i\"] = 0\n",
    "        logging[\"epoch\"] += 1\n",
    "        return features[ids[inner_i]:],labels[ids[inner_i]:]\n",
    "    else:\n",
    "        inner_i = logging[\"i\"]\n",
    "        logging[\"i\"] += 1\n",
    "        return features[ids[inner_i]:ids[logging[\"i\"]]],labels[ids[inner_i]:ids[logging[\"i\"]]]\n",
    "\n",
    "\n",
    "\n",
    "def sum_to_pred(row):\n",
    "    r = np.rint(np.sum(row))\n",
    "    if r>10:\n",
    "        return 10\n",
    "    elif r<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "train_data = [X_train[i,] for i in range(X_train.shape[0])]\n",
    "train_data = [np.array(each.todense())[0] for each in train_data]\n",
    "test_data = X_test\n",
    "test_data_features = [X_test[i,] for i in range(X_test.shape[0])]\n",
    "test_data_features = [np.array(each.todense())[0] for each in test_data_features]\n",
    "    \n",
    "def tuning(H,batch_size,num_features=X_train.shape[1]):   \n",
    "    global logging\n",
    "    logging[\"i\"] = 0\n",
    "    logging[\"epoch\"] = 0\n",
    "    ids = list(range(0,X_train.shape[0],batch_size))\n",
    "    print(\"feature:%i,H:%i,batch:%i\"%(num_features,H,batch_size))\n",
    "    #num_features = 5000\n",
    "\n",
    "    #batch_size = 10\n",
    "\n",
    "    #H = 5\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        x = tf.placeholder(tf.float32,shape=[None,num_features])\n",
    "        y_ = tf.placeholder(tf.float32,shape=[None,11,1])\n",
    "        new_y = tf.squeeze(y_)\n",
    "        W_1 = tf.Variable(tf.random_uniform([num_features,H]))\n",
    "        b_1 = tf.Variable(tf.zeros([H]))\n",
    "\n",
    "        W_2 = tf.Variable(tf.random_uniform([H,11]))\n",
    "        b_2 = tf.Variable(tf.zeros([11]))\n",
    "\n",
    "        temp_1 = tf.nn.relu(tf.matmul(x,W_1) + b_1)\n",
    "        temp_2 = tf.matmul(temp_1,W_2) + b_2\n",
    "        output = tf.nn.sigmoid(temp_2)\n",
    "        #loss = tf.reduce_mean(tf.square(tf.subtract(output,new_y)))\n",
    "        loss = tf.losses.mean_squared_error(output,new_y)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        opt_op = opt.minimize(loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "    # with tf.Session(graph = graph) as session:\n",
    "\n",
    "    sess = tf.InteractiveSession(graph=graph)\n",
    "    sess.run(init)\n",
    "    print(\"Initialized\")\n",
    "    step = 0 \n",
    "    while logging[\"epoch\"]<logging[\"epochs\"]:\n",
    "        step+=1\n",
    "        batch_x,batch_y = generate_batch(train_data,y_train,ids)\n",
    "        feed_dict={x:batch_x,y_:batch_y}\n",
    "        _,loss_val = sess.run([opt_op,loss],feed_dict=feed_dict)\n",
    "        if step % 2000 == 0:\n",
    "            print(\"Step: %i, Epoch: %i, Loss:%f\"%(step,logging[\"epoch\"],loss_val))\n",
    "\n",
    "    result = sess.run(output,feed_dict={x:test_data_features})\n",
    "    result_label = [to_predict(each) for each in result]\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    sd = mean_squared_error(result_label, y_test)\n",
    "\n",
    "\n",
    "    result = sess.run(output,feed_dict={x:test_data_features})\n",
    "    result_label = [sum_to_pred(each) for each in result]\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    custom = mean_squared_error(result_label, y_test)\n",
    "    sess.close()\n",
    "    return sd,custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuning(5,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
